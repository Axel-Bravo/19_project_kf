{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOIHZTkur5Mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Connection with Google Drive\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "#!cp -rv /content/train_processed.zip /content/gdrive/My\\ Drive/train_processed.zip\n",
        "#!rm /content/gdrive/My\\ Drive/kaggle_data/train_processed.zip\n",
        "#!cp -rv /content/gdrive/My\\ Drive/train_processed.zip /content/gdrive/My\\ Drive/kaggle_data/train_processed.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr6wHM4-GhY_",
        "colab_type": "text"
      },
      "source": [
        "# Connecting with Kaggle and Storing Data locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T99ST8l3ahNW",
        "colab_type": "code",
        "outputId": "93cd7279-9b27-40e3-ab19-a504625ff14b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Get data from Kaggle\n",
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mkdir /content/.kaggle\n",
        "!ls ~/.kaggle\n",
        "\n",
        "import json\n",
        "token = {\"username\":\"mielgosez\", \"key\":\"46ef90eb86d527d54aca090759ccbef9\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "    \n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "\n",
        "print(\"1. Downloading datasets\")\n",
        "!kaggle competitions download -c histopathologic-cancer-detection -p /content/\n",
        "\n",
        "print(\"2.  Moving train set to train folder\")\n",
        "!mkdir /content/train\n",
        "!unzip -qq /content/train.zip -d /content/train\n",
        "print(\"3. Moving test set to test folder\")\n",
        "!mkdir /content/test\n",
        "!unzip -qq /content/test.zip -d /content/test\n",
        "\n",
        "\n",
        "print(\"4. Moving files to their corresponding folders\")\n",
        "!mkdir /content/train/0\n",
        "!mkdir /content/train/1\n",
        "import pandas as pd\n",
        "import shutil\n",
        "!unzip -qq /content/train_labels.csv.zip\n",
        "file_assign = pd.read_csv(\"/content/train_labels.csv\") \n",
        "file_assign = file_assign.values\n",
        "for item in file_assign:\n",
        "  if item[1] == 0:\n",
        "    shutil.move(\"/content/train/\"+item[0]+\".tif\", \n",
        "                \"/content/train/0/\"+item[0]+\".tif\")\n",
        "  else:\n",
        "    shutil.move(\"/content/train/\"+item[0]+\".tif\", \n",
        "                \"/content/train/1/\"+item[0]+\".tif\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n",
            "- path is now set to: {/content}\n",
            "1. Downloading datasets\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.33M [00:00<?, ?B/s]\n",
            "100% 1.33M/1.33M [00:00<00:00, 43.6MB/s]\n",
            "Downloading train_labels.csv.zip to /content\n",
            " 98% 5.00M/5.10M [00:00<00:00, 10.5MB/s]\n",
            "100% 5.10M/5.10M [00:00<00:00, 10.4MB/s]\n",
            "Downloading test.zip to /content\n",
            " 99% 1.29G/1.30G [00:28<00:00, 46.5MB/s]\n",
            "100% 1.30G/1.30G [00:28<00:00, 49.4MB/s]\n",
            "Downloading train.zip to /content\n",
            "100% 4.98G/4.98G [02:01<00:00, 48.6MB/s]\n",
            "100% 4.98G/4.98G [02:01<00:00, 43.8MB/s]\n",
            "2.  Moving train set to train folder\n",
            "3. Moving test set to test folder\n",
            "4. Moving files to their corresponding folders\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3A84msOJqlR",
        "colab_type": "code",
        "outputId": "979b54ce-95be-459e-fe6f-da91df3f18b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Verify that the \n",
        "import os\n",
        "path, dirs, files = next(os.walk(\"/content/train/0\"))\n",
        "file_count = len(files)\n",
        "print(\"Number of files in train folder cat 0:\"+ str(file_count))\n",
        "\n",
        "path, dirs, files = next(os.walk(\"/content/train/1\"))\n",
        "file_count = len(files)\n",
        "print(\"Number of files in train folder cat 1:\"+ str(file_count))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of files in train folder cat 0:130908\n",
            "Number of files in train folder cat 1:89117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTggFeHNGu4K",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEEzEwv4hvbc",
        "colab_type": "code",
        "outputId": "ef6f1f29-11e6-45db-ada0-03c6845ee23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = \"/content/train\"\n",
        "test_dir = \"/content/test\"\n",
        "\n",
        "# Parameters\n",
        "image_size = 96\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Data Setting\n",
        "\n",
        "general_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n",
        "        vertical_flip=True,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2)\n",
        "\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x)\n",
        "\n",
        "# Aqu√≠ dejar la mariquera y poner todo boolean\n",
        "train_generator = general_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = general_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='validation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 176021 images belonging to 2 classes.\n",
            "Found 44004 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTYc6lwKi-cY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "file_assign = pd.read_csv(\"/content/train_labels.csv\") \n",
        "file_assign = file_assign.values\n",
        "\n",
        "image_sizes = []\n",
        "for item in file_assign:\n",
        "  if item[1] == 0:\n",
        "    im = Image.open(\"/content/test/\"+item[0]+\".tif\")\n",
        "  else:\n",
        "    im = Image.open(\"/content/train/1/\"+item[0]+\".tif\")\n",
        "  image_sizes.append(im.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QobiEUX89cyo",
        "colab_type": "code",
        "outputId": "d71f3090-9129-4cda-ff65-bc159fba5107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ps = pd.Series([tuple(i) for i in image_sizes])\n",
        "counts = ps.value_counts()\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 96)    220025\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IQ3WzBa-ZD0",
        "colab_type": "code",
        "outputId": "6b13f235-7d3d-411a-b087-f26b29603557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1210
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers import Conv2D, MaxPool2D\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "\n",
        "kernel_size = (3,3)\n",
        "pool_size= (2,2)\n",
        "first_filters = 32\n",
        "second_filters = 64\n",
        "third_filters = 128\n",
        "\n",
        "dropout_conv = 0.3\n",
        "dropout_dense = 0.5\n",
        "\n",
        "# Use sigmoid and binary cross-entropy to address the problem in a more accurate fashion.\n",
        "# Another trick is to increase the kernel size to track 'bigger' details\n",
        "# Batch normalization is:\n",
        "# \n",
        "model = Sequential()\n",
        "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (image_size, image_size, 3)))\n",
        "model.add(Conv2D(first_filters, kernel_size, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool2D(pool_size = pool_size)) \n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool2D(pool_size = pool_size))\n",
        "model.add(Dropout(dropout_conv))\n",
        "\n",
        "#model.add(GlobalAveragePooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, use_bias=False))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(dropout_dense))\n",
        "model.add(Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 94, 94, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 92, 92, 32)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 92, 92, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 92, 92, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 46, 46, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 46, 46, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 44, 44, 64)        18432     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 44, 44, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 44, 44, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 42, 42, 64)        36864     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 42, 42, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 42, 42, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 21, 21, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 19, 19, 128)       73728     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 19, 19, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 19, 19, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 17, 17, 128)       147456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 17, 17, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               2097152   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 2,386,689\n",
            "Trainable params: 2,385,345\n",
            "Non-trainable params: 1,344\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNGvScmlJ9CE",
        "colab_type": "code",
        "outputId": "9b4f940b-53aa-4f0b-9688-1334bf8d7857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "\n",
        "# 176021 and 44004 are the sizes provided in the bloc where train_generator\n",
        "# and validation_generator were defined.\n",
        "train_steps = np.ceil(176021 / batch_size)\n",
        "val_steps = np.ceil(44004 / batch_size)\n",
        "\n",
        "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
        "reducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n",
        "history = model.fit_generator(train_generator, steps_per_epoch=train_steps, \n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=val_steps,\n",
        "                    epochs=6,\n",
        "                    callbacks=[reducel, earlystopper])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "5501/5501 [==============================] - 2252s 409ms/step - loss: 0.3116 - acc: 0.8726 - val_loss: 0.2668 - val_acc: 0.8905\n",
            "Epoch 2/6\n",
            "5501/5501 [==============================] - 2144s 390ms/step - loss: 0.2695 - acc: 0.8934 - val_loss: 0.2593 - val_acc: 0.8929\n",
            "Epoch 3/6\n",
            "5501/5501 [==============================] - 2133s 388ms/step - loss: 0.2480 - acc: 0.9045 - val_loss: 0.2805 - val_acc: 0.8881\n",
            "\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 4/6\n",
            "5501/5501 [==============================] - 2153s 391ms/step - loss: 0.2071 - acc: 0.9221 - val_loss: 0.2529 - val_acc: 0.9041\n",
            "Epoch 5/6\n",
            "5501/5501 [==============================] - 2150s 391ms/step - loss: 0.1986 - acc: 0.9250 - val_loss: 0.2153 - val_acc: 0.9170\n",
            "Epoch 6/6\n",
            "5501/5501 [==============================] - 2145s 390ms/step - loss: 0.1938 - acc: 0.9273 - val_loss: 0.2056 - val_acc: 0.9212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87oeylvuV8CK",
        "colab_type": "code",
        "outputId": "69058404-a6ae-4429-d887-4cecda7c375c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7oU_uTvi8b8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e94de746-0193-4207-bf90-0c04623ee8b9"
      },
      "source": [
        "!ls /content/test/ | wc -l\n",
        "!unzip -qq /content/test.zip -d /content/test/test\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        \"/content/test/\",\n",
        "        target_size=(image_size, image_size),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=1,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "filenames = test_generator.filenames\n",
        "nb_samples = len(filenames)\n",
        "\n",
        "predict = model.predict_generator(test_generator,steps = nb_samples, verbose=1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57458\n",
            "Found 57458 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBWHWv3ZG2X6",
        "colab_type": "text"
      },
      "source": [
        "# Generating the report on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OittQHZl9iAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "da3346b6-de3d-4e09-ffef-8b9b4238178e"
      },
      "source": [
        "from glob import glob\n",
        "import os\n",
        "from skimage.io import imread\n",
        "\n",
        "base_test_dir = '/content/test/test/'\n",
        "test_files = glob(os.path.join(base_test_dir,'*.tif'))\n",
        "submission = pd.DataFrame()\n",
        "file_batch = 5000\n",
        "max_idx = len(test_files)\n",
        "for idx in range(0, max_idx, file_batch):\n",
        "    print(max_idx)\n",
        "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
        "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
        "    test_df['id'] = test_df.path.map(lambda x: x.split('/')[4].split(\".\")[0])\n",
        "    test_df['image'] = test_df['path'].map(imread)\n",
        "    K_test = np.stack(test_df[\"image\"].values)\n",
        "    K_test = (K_test - K_test.mean()) / K_test.std()\n",
        "    predictions = model.predict(K_test)\n",
        "    test_df['label'] = predictions\n",
        "    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\n",
        "submission.head()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57458\n",
            "Indexes: 0 - 5000\n",
            "57458\n",
            "Indexes: 5000 - 10000\n",
            "57458\n",
            "Indexes: 10000 - 15000\n",
            "57458\n",
            "Indexes: 15000 - 20000\n",
            "57458\n",
            "Indexes: 20000 - 25000\n",
            "57458\n",
            "Indexes: 25000 - 30000\n",
            "57458\n",
            "Indexes: 30000 - 35000\n",
            "57458\n",
            "Indexes: 35000 - 40000\n",
            "57458\n",
            "Indexes: 40000 - 45000\n",
            "57458\n",
            "Indexes: 45000 - 50000\n",
            "57458\n",
            "Indexes: 50000 - 55000\n",
            "57458\n",
            "Indexes: 55000 - 60000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b8c53202df10b76794c1daa23e8997e0ac9a6f94</td>\n",
              "      <td>0.237784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3a4c4b99fbff710accb62d5fa480d131e57845a5</td>\n",
              "      <td>0.053820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a98ab9e399ef57531448592c725bb6141dfb83e6</td>\n",
              "      <td>0.050331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45732e8cfd8d42b1fd1f97feaf748d3ba3e36474</td>\n",
              "      <td>0.013040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4eefeedfcc99914ee4ecb32bedfa93c19884d702</td>\n",
              "      <td>0.007161</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id     label\n",
              "0  b8c53202df10b76794c1daa23e8997e0ac9a6f94  0.237784\n",
              "1  3a4c4b99fbff710accb62d5fa480d131e57845a5  0.053820\n",
              "2  a98ab9e399ef57531448592c725bb6141dfb83e6  0.050331\n",
              "3  45732e8cfd8d42b1fd1f97feaf748d3ba3e36474  0.013040\n",
              "4  4eefeedfcc99914ee4ecb32bedfa93c19884d702  0.007161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDwlJm1RG-Ox",
        "colab_type": "text"
      },
      "source": [
        "Results are submitted to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zxdB591-cmS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d6f6054e-a2aa-4a54-f881-747b00e69b72"
      },
      "source": [
        "submission.to_csv(\"results.csv\",index=False)\n",
        "!kaggle competitions submit -c histopathologic-cancer-detection -f results.csv -m \"2nd attempt\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% 2.87M/2.87M [00:07<00:00, 407kB/s]\n",
            "Successfully submitted to Histopathologic Cancer Detection"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "816k-10xFU_s",
        "colab_type": "text"
      },
      "source": [
        "Accuracy: 0.9044"
      ]
    }
  ]
}